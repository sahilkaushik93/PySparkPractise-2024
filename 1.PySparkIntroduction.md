# Spark Introduction:-
1. Apache Spark is an open-source unified analytics engine used for large-scale data processing, hereafter referred it as Spark.
2. Spark runs operations on billions and trillions of data on distributed clusters 100 times faster than traditional applications, thus ideal for processing large-scale data sets.
3. Spark can: (run on single node or multi node machines a.k.a cluster thus addressing the limitation of MapReduce), (it can do in-memory processing using in-memory cahes), (can do real-time streaming).
4. Spark APIs: Scala(Spark's primary language), Java API-(Java), PySpark-(Python), R API(R)

# Pyspark Introduction:-
1. PySpark is a powerful open-source framework built on Apache Spark, designed to simplify and accelerate large-scale data processing and analytics tasks. PySpark enables developers to write Spark applications using Python.
2. Distributed computing model, capable of processing massive datasets across clusters of machines.
3. Ability to handle diverse data formats, support for complex analytics operations, and fault tolerance.
4. Scalability, faster processing times, and enhanced performance.
5. If you are working with a smaller Dataset and don’t have a Spark cluster, but still want to get benefits similar to Spark DataFrame, you can use Python Pandas DataFrames. The main difference is Pandas DataFrame is not distributed and runs on a single node while spark dataframe can run on multiple nodes across clusters a.k.a parallel processing capability.

# Pyspark Features:-
1. Distributed Computing: PySpark utilizes Spark’s distributed computing framework to process large-scale data across a cluster of machines, enabling parallel execution of tasks.
2. Fault Tolerance: Automatically handles fault tolerance by maintaining resilient distributed datasets (RDDs), which allows it to recover from failures gracefully.
3. Lazy Evaluation: PySpark employs lazy evaluation, meaning transformations on data are not executed immediately but rather stored as a directed acyclic graph (DAG) of computations until an action is triggered.
4. Real-time Processing: With Streaming and Structured Streaming, PySpark enables real-time processing of data streams, facilitating timely insights and responses to changing data.
5. Machine Learning Capabilities: Includes MLlib, machine learning library, providing scalable implementations of popular machine learning algorithms, allowing for large-scale model training and deployment.
